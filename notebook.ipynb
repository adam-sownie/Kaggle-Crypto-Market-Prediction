{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f6cfc6-a80b-48ab-a4e9-d8c4ef1808b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "%pip install fastparquet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import fastparquet\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1167f8b-b718-4323-858d-68dd57cf28a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    df = pd.read_parquet('train.parquet')\n",
    "    print(\"Successfully loaded parquet file with existing engine\")\n",
    "except ImportError as e:\n",
    "    print(\"ParquetEngine not found. Installing pyarrow...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyarrow\"])\n",
    "    print(\"pyarrow installed. Attempting to load parquet file again...\")\n",
    "    df = pd.read_parquet('train.parquet')\n",
    "except FileNotFoundError:\n",
    "    print(\"train.parquet not found. Please ensure the file is in the current directory.\")\n",
    "    print(\"Alternative: If you have a CSV file, change the filename below:\")\n",
    "    # df = pd.read_csv('train.csv')  # Uncomment and modify if using CSV\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error loading parquet file: {e}\")\n",
    "    print(\"Trying with fastparquet engine...\")\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"fastparquet\"])\n",
    "        df = pd.read_parquet('train.parquet', engine='fastparquet')\n",
    "    except:\n",
    "        print(\"Failed to load with fastparquet. Please install manually:\")\n",
    "        print(\"Run: pip install pyarrow\")\n",
    "        print(\"Or: pip install fastparquet\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4068902c-dfe3-4fdc-971f-bd01b1ffe1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print data\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a946ab5a-831b-46a4-9f06-8169540a3295",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original shape: {df.shape}\")\n",
    "\n",
    "# Store initial dimensions\n",
    "initial_rows = df.shape[0]\n",
    "initial_cols = df.shape[1]\n",
    "\n",
    "# Drop specified columns (mix of unique and duplicate columns)\n",
    "cols_to_drop = ['X697', 'X698', 'X699', 'X700', 'X701', 'X702', 'X703', 'X704', 'X705', 'X706', 'X707', 'X708', 'X709', 'X710', 'X711', 'X712', 'X713', 'X714', 'X715', 'X716', 'X717', 'X864', 'X867', 'X869', 'X870', 'X871', 'X872'] + ['X104', 'X146', 'X110', 'X152', 'X116', 'X158', 'X122', 'X164', 'X128', 'X170', 'X134', 'X176', 'X140', 'X182', 'X351', 'X393', 'X357', 'X399', 'X363', 'X405', 'X369', 'X411', 'X375', 'X417', 'X381', 'X423', 'X387', 'X429']\n",
    "\n",
    "# Only drop columns that actually exist in the dataframe\n",
    "cols_to_drop_existing = [col for col in cols_to_drop if col in df.columns]\n",
    "df.drop(columns=cols_to_drop_existing, inplace=True)\n",
    "\n",
    "print(f\"Dropped {len(cols_to_drop_existing)} specified columns\")\n",
    "\n",
    "# Drop duplicate rows\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82c2ff9-08c1-49f9-8eba-fdce6f62a49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data exploration\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA EXPLORATION\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nBasic statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b5fc8-2969-4bb6-83e6-c613aaca2e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_count = df.isnull().sum().sum()\n",
    "print(f\"Missing values found: {missing_count}\")\n",
    "\n",
    "if missing_count > 0:\n",
    "    print(\"Handling missing values...\")\n",
    "    \n",
    "    # Check for columns that are entirely NaN\n",
    "    all_nan_cols = df.columns[df.isnull().all()].tolist()\n",
    "    if all_nan_cols:\n",
    "        print(f\"Found {len(all_nan_cols)} columns with all NaN values - dropping them\")\n",
    "        df = df.drop(columns=all_nan_cols)\n",
    "    \n",
    "    # Handle remaining missing values\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            if df[col].dtype in ['object', 'category']:\n",
    "                mode_val = df[col].mode()\n",
    "                df[col] = df[col].fillna(mode_val[0] if len(mode_val) > 0 else 'unknown')\n",
    "            else:\n",
    "                # Check if column has any non-NaN values before calculating median\n",
    "                non_nan_count = df[col].count()\n",
    "                if non_nan_count > 0:\n",
    "                    median_val = df[col].median()\n",
    "                    df[col] = df[col].fillna(median_val)\n",
    "                else:\n",
    "                    # If somehow still all NaN, fill with 0\n",
    "                    df[col] = df[col].fillna(0)\n",
    "    print(\"Missing values filled\")\n",
    "else:\n",
    "    print(\"No missing values found\")\n",
    "\n",
    "# Check for infinite values\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "inf_count = np.isinf(df[numeric_cols]).sum().sum()\n",
    "print(f\"Infinite values found: {inf_count}\")\n",
    "\n",
    "if inf_count > 0:\n",
    "    print(\"Handling infinite values...\")\n",
    "    # Replace inf with NaN, then fill with median\n",
    "    df[numeric_cols] = df[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            # Check if column has any non-NaN values before calculating median\n",
    "            non_nan_count = df[col].count()\n",
    "            if non_nan_count > 0:\n",
    "                median_val = df[col].median()\n",
    "                df[col] = df[col].fillna(median_val)\n",
    "            else:\n",
    "                # If all NaN, fill with 0\n",
    "                df[col] = df[col].fillna(0)\n",
    "    print(\"Infinite values replaced with median values\")\n",
    "else:\n",
    "    print(\"No infinite values found\")\n",
    "\n",
    "# Check for very large values that might cause issues\n",
    "large_value_threshold = 1e10\n",
    "large_values = (np.abs(df[numeric_cols]) > large_value_threshold).sum().sum()\n",
    "print(f\"Very large values (>{large_value_threshold}): {large_values}\")\n",
    "\n",
    "if large_values > 0:\n",
    "    print(\"Clipping very large values...\")\n",
    "    for col in numeric_cols:\n",
    "        df[col] = np.clip(df[col], -large_value_threshold, large_value_threshold)\n",
    "    print(\"Large values clipped to reasonable range\")\n",
    "else:\n",
    "    print(\"No extremely large values found\")\n",
    "\n",
    "print(\"Data preprocessing complete - ready for XGBoost training\")\n",
    "print(f\"Final dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6100f05-7fc1-475f-99c3-ac02b0b6744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "target_col = 'label'  # Target column name\n",
    "if target_col not in df.columns:\n",
    "    print(f\"Target column '{target_col}' not found. Available columns: {list(df.columns)}\")\n",
    "    raise ValueError(f\"Column '{target_col}' not found in dataset\")\n",
    "\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7463b8e1-fe30-46a9-b8d3-92f4f778f9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target if it's categorical\n",
    "if y.dtype == 'object' or y.dtype.name == 'category':\n",
    "    target_encoder = LabelEncoder()\n",
    "    y = target_encoder.fit_transform(y)\n",
    "    print(f\"Target classes: {target_encoder.classes_}\")\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Target distribution:\\n{pd.Series(y).value_counts()}\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bfc5a6-db80-419d-80f1-7db551b9c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Train XGBoost model\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"XGBOOST TRAINING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric='rmse'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05faee5-768a-4e91-bc9c-ffe27b7bdbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "    \n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "    \n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(xgb_model, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "print(f\"\\nCross-validation RMSE: {-cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aec0612-368d-43b5-a3e3-bfb82aa9a7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FEATURE IMPORTANCE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 most important features:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 15 Feature Importances')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f5e136-d733-4657-966d-98bcb2dcc78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAVING MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "import joblib\n",
    "joblib.dump(xgb_model, 'xgboost_model.pkl')\n",
    "print(\"Model saved as 'xgboost_model.pkl'\")\n",
    "print(\"(No label encoders needed - all numeric data)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
